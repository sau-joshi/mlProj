---
title: "Predicting Activity Quality from Activity Monitors"
output:
  html_document:
    keep_md: true
---

## Synopsis

The objective is to use machine learning to predict activity quality from activity monitors. The data used for this project is obtained through the machine learning coursera course from JHU. The online reference is: <http://groupware.les.inf.puc-rio.br/har>



## Data

#### Data download
Data is download from the Coursera site and saved locally which is then read into R. Since data is not clean, and care must be taken to distinguish improbable data.

```{r inputChunk, cache=TRUE}
# uncomment to download to and read from local file
#
#urlTrain <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
#urlTest <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#download.file(urlTrain, destfile="datTrain.csv")
#download.file(urlTest, destfile="datTest.csv")

dat.training <- read.csv("datTrain.csv", na.strings = c("NA"," ","#DIV/0!"), strip.white=T)
```


#### Data exploration

Here are some of the observations on the data:

- Initial exploration of training data as shown by the first graph shows that quality of data (variable `classe`) is a function of data series (variable `X`), which does not really make sense. There are other irrelevant data also, which will be culled later.
```{r}
sapply(levels(dat.training$classe), function(x) nrow(subset(dat.training,dat.training$classe==x)))
```


- Similarly, the variable `new_window` is uniformly distributed over `classe` and therefore, we can disregard this variable for prediction. This is shown in the second figure.

- Variables like time-stamps and usernames are also irrelevant to prediction.

```{r}
boxplot(dat.training$X ~ dat.training$classe) 
plot(dat.training$new_window ~ dat.training$classe)
boxplot(dat.training$num_window ~ dat.training$classe)
```

- There are also a large number of variables with missing values. Since more than 95% of the data is missing in the variables, there is not sensible way to impute the data. Hence such variables are also removed from the training data.

These data manipulations are shown in the next section. The third figure shows a box plot one of the many variables that affect `classe`.

## Data Manipulation

First of all, the variables with improbable data and incorrect formats are eliminated. Then, 91 variables with more than 95% missing data is removed. Finally, the clean training data set has 19622 rows and 54 columns (including `classe`).

```{r dataChunk, cache=TRUE, dependson="inputChunk"}

# serial number, usernametime stamps, and other zero variables can be removed
names <- data.frame(names(dat.training))
colNameNuseful <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
                  "cvtd_timestamp", "new_window", "kurtosis_yaw_belt", "skewness_yaw_belt",
                  "amplitude_yaw_belt", "kurtosis_yaw_dumbbell", "skewness_yaw_dumbbell",
                  "amplitude_yaw_dumbbell", "kurtosis_yaw_forearm", "skewness_yaw_forearm",
                  "amplitude_yaw_forearm")
colNuseful <- sapply(colNameNuseful, function(x) which(names==x))
  # unlist(colNuseful) = c(1:6,14,17,26,89,92,127,130,139)
training <- dat.training[,-unlist(colNuseful)]

# remove columns with NA values 
colNA <- sapply(1:ncol(training), 
                function(x) ifelse(is.na(summary(training[,x])[7]),TRUE,FALSE) ) 
# Columns with more than 95% missing data
sum(colNA==FALSE)
training <- training[,colNA]
    # dim(training) gives 19622 rows and 54 columns

```


## Model

#### Creating testing and training data

For the model, first a random sample of 20 rows is taken as a test set (similar to the quiz set in the Netflix competition). The rest of the training dataset is used for training.

```{r modelChunk, cache=TRUE, dependson="dataChunk"}
set.seed(6834)

index.test <- sample(1:nrow(training),size=20)
dat.test <- training[index.test,]
dat.train <- training[-index.test,]

```

#### Prediction using decision tree (`rpart`)

First a decision tree model was created and tested using `rpart` package in R.
```{r initialmodelChunk, cache=TRUE, dependson="dataChunk"}
require(rpart)

# creating model
fitrpart <- rpart(dat.train$classe ~ ., data=dat.train) 

# applying prediction model on test data
pred <- predict(fitrpart,dat.test) 

# checking accuracy
table(dat.test$classe, sapply(1:20, function(x) names(which.max(pred[x,])))) 
```

This model did not give the correct classification in the test data, as shown above.


#### Prediction using `randomForest`

The randomForest machine learning algorithm is used on the training data to create the prediction model.

```{r trainChunk, cache=TRUE, dependson="modelChunk"}
require(randomForest)
rf <- randomForest(classe ~ ., data = dat.train, ntree = 500) 
rf
```

#### Cross-validation


We can see that the prediction model gives complete accuracy on the small sample of testing data.

```{r}
table(dat.test$classe,predict(rf,dat.test))
```


#### Expected output sample error

From the randomForest model above, we can expect an error rate of `0.13%`, which is very impressive compared to the previous decision tree.

## Prediction on Testing

#### Reading and preparing test data set

Like the training dataset, the testing data needs to be cleaned as follows:

```{r, cache=TRUE}
dat.testing <- read.csv("datTest.csv", na.strings = c("NA"," ","#DIV/0!"), strip.white=T)

testing <- dat.testing[,-unlist(colNuseful)]
testing <- testing[,colNA]
```

Running the prediction model on testing data, we get:
```{r}
require(randomForest)
answers <- as.character(predict(rf,testing[-ncol(testing)]))
answers

```



#### Submission

The predictions to the test set was created as a character vector. A function was supplied by the coursera course to create files for each of the predictions.

```{r, eval=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("output_submission/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

# Uncomment to write answers to the `output_submission` folder
#
# pml_write_files(answers)
```

## Conclusion

The model developed using randomForest package gives correct results for the submission on the test set. However, I feel there could have been many more refinements in the model as well as the process followed. 

1. For example, I had tried to follow the lecture videos and used principal component analysis to summarize the number of variables. However, there is the issue of interpretability with so many variables used arbitrarily even though the model gave more accuracy.

2. One of the biggest challenges was hardware capability to generate prediction models. I tried principal component analysis and then tree partition, both of which failed due to lack of memory. I attribute this problem to my lack of adequate knowledge on the selection of appropriate parameters to use caret package efficiently. This in turn is due to my lack of in-depth knowledge in machine learning methods.

Overall, I have learnt that machine learning methods are a powerful way to solve complex and massive real-life classification problems even though interpretability (intuition) is is a challenge.
